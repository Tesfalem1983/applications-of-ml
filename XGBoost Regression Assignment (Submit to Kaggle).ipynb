{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment is inspired by: \n",
    "\n",
    "- https://www.kaggle.com/code/carlmcbrideellis/an-introduction-to-xgboost-regression\n",
    "- https://www.kaggle.com/code/dansbecker/xgboost/notebook\n",
    "\n",
    "In this assignment we will apply XGBoost Regression techniques to predict house prices, based on the famous Kaggle Dataset https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques\n",
    "\n",
    "Step 1 is to download the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_cell_guid": "e7842527-0531-44fa-8ce0-ae4cc3cfd0d7",
    "_uuid": "365f3e96-c7a9-41cf-9c5f-76dbfd46168c",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Shape: (1460, 80)\n",
      "Test Data Shape: (1459, 79)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1460 entries, 1 to 1460\n",
      "Data columns (total 36 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   MSSubClass     1460 non-null   int64  \n",
      " 1   LotFrontage    1460 non-null   float64\n",
      " 2   LotArea        1460 non-null   int64  \n",
      " 3   OverallQual    1460 non-null   int64  \n",
      " 4   OverallCond    1460 non-null   int64  \n",
      " 5   YearBuilt      1460 non-null   int64  \n",
      " 6   YearRemodAdd   1460 non-null   int64  \n",
      " 7   MasVnrArea     1460 non-null   float64\n",
      " 8   BsmtFinSF1     1460 non-null   int64  \n",
      " 9   BsmtFinSF2     1460 non-null   int64  \n",
      " 10  BsmtUnfSF      1460 non-null   int64  \n",
      " 11  TotalBsmtSF    1460 non-null   int64  \n",
      " 12  1stFlrSF       1460 non-null   int64  \n",
      " 13  2ndFlrSF       1460 non-null   int64  \n",
      " 14  LowQualFinSF   1460 non-null   int64  \n",
      " 15  GrLivArea      1460 non-null   int64  \n",
      " 16  BsmtFullBath   1460 non-null   int64  \n",
      " 17  BsmtHalfBath   1460 non-null   int64  \n",
      " 18  FullBath       1460 non-null   int64  \n",
      " 19  HalfBath       1460 non-null   int64  \n",
      " 20  BedroomAbvGr   1460 non-null   int64  \n",
      " 21  KitchenAbvGr   1460 non-null   int64  \n",
      " 22  TotRmsAbvGrd   1460 non-null   int64  \n",
      " 23  Fireplaces     1460 non-null   int64  \n",
      " 24  GarageYrBlt    1460 non-null   float64\n",
      " 25  GarageCars     1460 non-null   int64  \n",
      " 26  GarageArea     1460 non-null   int64  \n",
      " 27  WoodDeckSF     1460 non-null   int64  \n",
      " 28  OpenPorchSF    1460 non-null   int64  \n",
      " 29  EnclosedPorch  1460 non-null   int64  \n",
      " 30  3SsnPorch      1460 non-null   int64  \n",
      " 31  ScreenPorch    1460 non-null   int64  \n",
      " 32  PoolArea       1460 non-null   int64  \n",
      " 33  MiscVal        1460 non-null   int64  \n",
      " 34  MoSold         1460 non-null   int64  \n",
      " 35  YrSold         1460 non-null   int64  \n",
      "dtypes: float64(3), int64(33)\n",
      "memory usage: 422.0 KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read the data\n",
    "train_data = pd.read_csv('C:/Users/simon/applications-of-ml/house-prices-advanced-regression-techniques/train.csv', index_col=0)\n",
    "test_data = pd.read_csv('C:/Users/simon/applications-of-ml/house-prices-advanced-regression-techniques/test.csv', index_col=0)\n",
    "target= 'SalePrice'\n",
    "\n",
    "# Print the shapes of the data frames\n",
    "print(\"Train Data Shape:\", train_data.shape)\n",
    "print(\"Test Data Shape:\", test_data.shape)\n",
    "\n",
    "\n",
    "X_train = train_data.select_dtypes(include=['number']).copy()\n",
    "X_train = X_train.drop([target], axis=1)\n",
    "y_train = train_data[target]\n",
    "X_test  = test_data.select_dtypes(include=['number']).copy()\n",
    "y_test = test_data['SalePrice']\n",
    "\n",
    "# simple preprocessing: imputation; substitute any 'NaN' with mean value\n",
    "X_train = X_train.fillna(X_train.mean())\n",
    "X_test  = X_test.fillna(X_test.mean())\n",
    "\n",
    "\n",
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center style=\"background-color:Gainsboro; width:60%;\">Feature selection</center>\n",
    "The purpose of feature selection, as the name suggests, is to only model the most pertinent and important features, thus reducing the computational overhead, and also to alleviate the [curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality). The following are a number of notebooks covering techniques to achieve said goal, all of which use the House Prices data as an example:\n",
    "\n",
    "* [Feature selection using the Boruta-SHAP package](https://www.kaggle.com/carlmcbrideellis/feature-selection-using-the-boruta-shap-package)\n",
    "* [Recursive Feature Elimination (RFE) example](https://www.kaggle.com/carlmcbrideellis/recursive-feature-elimination-rfe-example)\n",
    "* [House Prices: Permutation Importance example](https://www.kaggle.com/carlmcbrideellis/house-prices-permutation-importance-example)\n",
    "* [Feature importance using the LASSO](https://www.kaggle.com/carlmcbrideellis/feature-importance-using-the-lasso)\n",
    "\n",
    "In this assignment, we shall use all of the numerical columns, and ignore the categorical features. To encode the categorical features one can use for example [pandas.get_dummies](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html). \n",
    "\n",
    "Our first task is to do Feature Exploration and Selection. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Recursive Feature Elimination (RFE) example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Feature Names: ['OverallQual', 'BsmtFinSF1', 'TotalBsmtSF', '2ndFlrSF', 'GrLivArea', 'LotFrontage', 'OverallQual', 'YearBuilt', 'BsmtFinSF1', 'LowQualFinSF']\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# Assuming you have a list of original feature names that matches the number of features\n",
    "original_feature_names = ['MSSubClass','LotFrontage','LotArea','OverallQual', 'OverallCond','YearBuilt','YearRemodAdd','MasVnrArea', \n",
    "                            'BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','1stFlrSF','2ndFlrSF','LowQualFinSF','GrLivArea', \n",
    "                            'BsmtFullBath' ,'BsmtHalfBath','FullBath','HalfBath', 'MSSubClass','LotFrontage','LotArea','OverallQual',\n",
    "                            'OverallCond','YearBuilt','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','1stFlrSF','2ndFlrSF',\n",
    "                            'LowQualFinSF','GrLivArea','BsmtFullBath','BsmtHalfBath','FullBath','HalfBath','BedroomAbvGr', 'KitchenAbvGr',\n",
    "                            'TotRmsAbvGrd','Fireplaces','GarageYrBlt','GarageCars','GarageArea','WoodDeckSF','OpenPorchSF','EnclosedPorch',\n",
    "                            '3SsnPorch','ScreenPorch',\t'PoolArea',\t'MiscVal','MoSold','YrSold']  \n",
    "\n",
    "# Choose the XGBoost regression model\n",
    "model = xgb.XGBRegressor()\n",
    "\n",
    "# Specify the number of features to select (you can adjust this as needed)\n",
    "num_features_to_select = 10\n",
    "\n",
    "# Initialize the RFE object\n",
    "rfe = RFE(model, n_features_to_select=num_features_to_select)\n",
    "\n",
    "# Fit RFE to the training data\n",
    "rfe.fit(X_train, y_train)\n",
    "\n",
    "# Get the selected feature indices\n",
    "selected_feature_indices = np.where(rfe.support_)[0]\n",
    "\n",
    "# Map the indices to feature names\n",
    "selected_feature_names = [original_feature_names[idx] for idx in selected_feature_indices]\n",
    "\n",
    "# Print the selected feature names\n",
    "print(\"Selected Feature Names:\", selected_feature_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center style=\"background-color:Gainsboro; width:60%;\">Feature engineering</center>\n",
    "As mentioned, one aspect of feature engineering is the creation of new features out of existing features. A simple example would be to create a new feature which is the sum of the number of bathrooms in the house:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in (X_train, X_test):\n",
    "    df[\"n_bathrooms\"] = df[\"BsmtFullBath\"] + (df[\"BsmtHalfBath\"]*0.5) + df[\"FullBath\"] + (df[\"HalfBath\"]*0.5)\n",
    "    df[\"area_with_basement\"]  = df[\"GrLivArea\"] + df[\"TotalBsmtSF\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your next task is to apply some feature engineering to prepare for using the XGBoost Estimator to predict house prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more on this fascinating aspect may I recommend the free on-line book [\"*Feature Engineering and Selection: A Practical Approach for Predictive Models*\"](http://www.feat.engineering/) by Max Kuhn and Kjell Johnson.\n",
    "### <center style=\"background-color:Gainsboro; width:60%;\">XGBoost estimator</center>\n",
    "Note that for this competition we use the RMSLE evaluation metric, rather than the default metric, which for regression is the RMSE. For more on the peculiarities of the RMSLE see the Appendix below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "_cell_guid": "e7842527-0531-44fa-8ce0-ae4cc3cfd0d7",
    "_uuid": "365f3e96-c7a9-41cf-9c5f-76dbfd46168c"
   },
   "outputs": [],
   "source": [
    "#=========================================================================\n",
    "# XGBoost regression: \n",
    "# Parameters: \n",
    "# n_estimators  \"Number of gradient boosted trees. Equivalent to number \n",
    "#                of boosting rounds.\"\n",
    "# learning_rate \"Boosting learning rate (also known as “eta”)\"\n",
    "# max_depth     \"Maximum depth of a tree. Increasing this value will make \n",
    "#                the model more complex and more likely to overfit.\" \n",
    "#=========================================================================\n",
    "regressor=xgb.XGBRegressor(eval_metric='rmsle')\n",
    "\n",
    "#=========================================================================\n",
    "# exhaustively search for the optimal hyperparameters\n",
    "#=========================================================================\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# set up our search grid\n",
    "param_grid = {\"max_depth\":    [4, 5],\n",
    "              \"n_estimators\": [500, 600, 700],\n",
    "              \"learning_rate\": [0.01, 0.015]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you use grid search to find the optimal hyper parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "Best Hyperparameters: {'learning_rate': 0.015, 'max_depth': 4, 'n_estimators': 700}\n"
     ]
    }
   ],
   "source": [
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(model, param_grid, cv=3, verbose=2, n_jobs=-1)\n",
    "\n",
    "# Perform hyperparameter tuning\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, can you setup a XGBoost Regressor object using your hyperparameters and fit it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "_cell_guid": "e7842527-0531-44fa-8ce0-ae4cc3cfd0d7",
    "_uuid": "365f3e96-c7a9-41cf-9c5f-76dbfd46168c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=0.015, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=4, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             n_estimators=700, n_jobs=None, num_parallel_tree=None,\n",
       "             predictor=None, random_state=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=0.015, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=4, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             n_estimators=700, n_jobs=None, num_parallel_tree=None,\n",
       "             predictor=None, random_state=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=0.015, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=4, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             n_estimators=700, n_jobs=None, num_parallel_tree=None,\n",
       "             predictor=None, random_state=None, ...)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize XGBoost Regressor with best hyperparameters\n",
    "from xgboost import XGBRegressor\n",
    "best_model = XGBRegressor(max_depth=best_params['max_depth'],\n",
    "                          n_estimators=best_params['n_estimators'],\n",
    "                          learning_rate=best_params['learning_rate'])\n",
    "\n",
    "# Fit the model to the training data\n",
    "best_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, can you run it on your test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "y_pred = best_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Root Mean Squared Logarithmic Error (RMSLE)\n",
    "def rmsle(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square(np.log1p(y_true) - np.log1p(y_pred))))\n",
    "\n",
    "rmsle_score = rmsle(y_test, y_pred)\n",
    "print(\"RMSLE with Best Model:\", rmsle_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Can you score your solution offline and see how it does?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # read in the ground truth file\n",
    "# solution   = pd.read_csv(<your solution file>)\n",
    "# y_true     = solution[\"SalePrice\"]\n",
    "\n",
    "# from sklearn.metrics import mean_squared_log_error\n",
    "# RMSLE = np.sqrt( mean_squared_log_error(y_true, predictions) )\n",
    "\n",
    "# print(\"The score is %.5f\" % RMSLE )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, use the below block to prepare your submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "_cell_guid": "e7842527-0531-44fa-8ce0-ae4cc3cfd0d7",
    "_uuid": "365f3e96-c7a9-41cf-9c5f-76dbfd46168c"
   },
   "outputs": [],
   "source": [
    "output = pd.DataFrame({\"Id\":test_data.index, \"SalePrice\":predictions})\n",
    "output.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center style=\"background-color:Gainsboro; width:60%;\">Feature importance</center>\n",
    "Let us also take a very quick look at the feature importance too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import plot_importance\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "plot_importance(regressor, max_num_features=8, ax=ax)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where here the `F score` is a measure \"*...based on the number of times a variable is selected for splitting, weighted by the squared improvement to the model as a result of each split, and averaged over all trees*.\" [1] \n",
    "\n",
    "Note that these importances are susceptible to small changes in the training data, and it is much better to make use of [\"GPU accelerated SHAP values\"](https://www.kaggle.com/carlmcbrideellis/gpu-accelerated-shap-values-jane-street-example), incorporated with version 1.3 of XGBoost.\n",
    "\n",
    "Can you follow the above guide use SHAP values instead of F Score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# Create an explainer object\n",
    "explainer = shap.Explainer(best_model)\n",
    "\n",
    "# Calculate SHAP values for the test data\n",
    "shap_values = explainer(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center style=\"background-color:Gainsboro; width:60%;\">Appendix: The RMSLE evaluation metric</center>\n",
    "From the competition [evaluation page](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview/evaluation) we see that the metric we are using is the root mean squared logarithmic error (RMSLE), which is given by\n",
    "\n",
    "$$ {\\mathrm {RMSLE}}\\,(y, \\hat y) = \\sqrt{ \\frac{1}{n} \\sum_{i=1}^n \\left(\\log (1 + \\hat{y}_i) - \\log (1 + y_i)\\right)^2} $$\n",
    "\n",
    "where $\\hat{y}_i$ is the predicted value of the target for instance $i$, and $y_i$\n",
    "is the actual value of the target for instance $i$.\n",
    "\n",
    "It is important to note that, unlike the RMSE, the RMSLE is asymmetric; penalizing much more the underestimated predictions than the overestimated predictions. For example, say the correct value is $y_i = 1000$, then underestimating by 600 is almost twice as bad as overestimating by 600:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSLE score is 0.915\n",
      "The RMSLE score is 0.470\n"
     ]
    }
   ],
   "source": [
    "def RSLE(y_hat,y):\n",
    "    return np.sqrt((np.log1p(y_hat) - np.log1p(y))**2)\n",
    "\n",
    "print(\"The RMSLE score is %.3f\" % RSLE( 400,1000) )\n",
    "print(\"The RMSLE score is %.3f\" % RSLE(1600,1000) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The asymmetry arises because \n",
    "\n",
    "$$ \\log (1 + \\hat{y}_i) - \\log (1 + y_i) =  \\log \\left( \\frac{1 + \\hat{y}_i}{1 + y_i} \\right) $$\n",
    "\n",
    "so we are essentially looking at ratios, rather than differences such as is the case of the RMSE. We can see the form that this asymmetry takes in the following plot, again using 1000 as our ground truth value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (7, 4)\n",
    "x = np.linspace(5,4000,100)\n",
    "plt.plot(x, RSLE(x,1000))\n",
    "plt.xlabel('prediction')\n",
    "plt.ylabel('RMSLE')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
